# Halucination Check Agent

![Agent Flow Diagram](assets/agent_flow.png)

- LLM ì‘ë‹µ ë‚´ í• ë£¨ì‹œë„¤ì´ì…˜(ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì •ë³´)ê³¼ ìê¸°ëª¨ìˆœ ì—¬ë¶€ë¥¼ ê²€ì¶œí•˜ëŠ” í‰ê°€ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
- **Context7** íˆ´ì€ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

## ğŸ“š Reference

* [SelfCheckAgent(25.02)](https://arxiv.org/html/2502.01812v1)
* [SelfCheckGPT(23.10)](https://github.com/potsawee/selfcheckgpt)
* [NLI ê¸°ë°˜ ë¬¸ì¥ ìê¸°ëª¨ìˆœ ë° í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ ë°©ë²• ìš”ì•½](assets/what_is_nli.md)

## ê³ ë¯¼ ë° ì„¤ê³„ ê²°ì •

### ê¸´ ë¬¸ì¥ë„ ê°€ëŠ¥í•œê°€?

* ì˜ˆì œëŠ” ëŒ€ë¶€ë¶„ ì§§ì§€ë§Œ, ëŒ€í‘œì ì¸ í• ë£¨ì‹œë„¤ì´ì…˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹([wiki\_bio\_gpt3\_hallucination](https://huggingface.co/datasets/potsawee/wiki_bio_gpt3_hallucination))ì˜ Ground TruthëŠ” ìƒë‹¹íˆ ê¸´ ë¬¸ì¥ë„ í¬í•¨.
* ë”°ë¼ì„œ ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„œë„ í‰ê°€ê°€ ê°€ëŠ¥í•˜ë‹¤ê³  ìƒê°.

### NLI vs Ngram vs LLM-API

* **LLM-API ê¸°ë°˜**ì€ ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ ì„±ëŠ¥ì´ ê°€ì¥ ìš°ìˆ˜í•˜ê³ , ì™œ ê·¸ëŸ° í‰ê°€ë¥¼ ë‚´ë ¸ëŠ”ì§€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ reasoning ì •ë³´ë„ í•¨ê»˜ ì œê³µ ê°€ëŠ¥.
* ë‹¨, LLM í˜¸ì¶œ ì‹¤íŒ¨(fallback) ê°€ëŠ¥ì„±ì€ ì¡´ì¬.
* **ì†ë„ê°€ ì¤‘ìš”í•˜ê±°ë‚˜ ë„ë©”ì¸ íŠ¹í™”** í™˜ê²½ì´ë¼ë©´ ì‘ì€ NLI ëª¨ë¸ì„ íŠœë‹í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ë„ ê³ ë ¤ ê°€ëŠ¥.

---

## ğŸ’» How to Run

### 1. í´ë¡  ë° ì˜ì¡´ì„± ì„¤ì¹˜

```bash
git clone https://github.com/middlek/halucination_check_agent.git
cd halucination_check_agent
uv venv
source .venv/bin/activate
uv sync
```

### 2. Langfuse ì‹¤í–‰

```bash
cd langfuse
docker compose up -d
```

Langfuseê°€ `http://localhost:3000`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë¡œê·¸ì¸ í›„ í”„ë¡œì íŠ¸ ìƒì„± ë° API í‚¤ ë°œê¸‰.

### 3. í™˜ê²½ë³€ìˆ˜ ì„¤ì •

```bash
# Copy example environment file and configure
cp .env.example .env

# Edit .env file with your API keys:
# - OpenAI API key (required)
# - Tavily API key (required for search mode)  
# - Langfuse keys (required for observability)
```

### 4. ì„œë²„ ì‹¤í–‰

#### A2A í”„ë¡œí† ì½œ ì„œë²„:

```bash
uv run src/server/app_a2a.py
```

#### FastA2A ì„œë²„ (ê°„ë‹¨í•œ ë²„ì „):

```bash
uv run src/server/app_fasta2a.py
```

ì„œë²„ ì‹¤í–‰ í›„ `http://localhost:8008/docs`ì—ì„œ API í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ğŸ”§ Basic Concepts

### NLI ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¶œ

ì´ ì‹œìŠ¤í…œì€ **Natural Language Inference(NLI)** ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ í• ë£¨ì‹œë„¤ì´ì…˜ì„ ê²€ì¶œí•©ë‹ˆë‹¤:

1. **ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘**: Tavily ê²€ìƒ‰ ë˜ëŠ” ì‚¬ìš©ì ì œê³µ ì»¨í…ìŠ¤íŠ¸
2. **ì¼ê´€ì„± í‰ê°€**: ì»¨í…ìŠ¤íŠ¸ì™€ ê²€ì¦í•  ë¬¸ì¥ ê°„ì˜ ë…¼ë¦¬ì  ê´€ê³„ ë¶„ì„
3. **ì ìˆ˜ ì‚°ì¶œ**: 0.0(ì¼ì¹˜) ~ 1.0(ëª¨ìˆœ) ì‚¬ì´ì˜ í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜
4. **ì´ìœ  ì œê³µ**: í‰ê°€ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…

### ë‘ ê°€ì§€ í‰ê°€ ëª¨ë“œ

**ê²€ìƒ‰ ëª¨ë“œ (Search Mode)**:
- Tavily APIë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
- ì°¬ì„±(pros)/ë°˜ëŒ€(cons) ê´€ì  ì§€ì›
- ì¼ë°˜ì ì¸ ì‚¬ì‹¤ í™•ì¸ì— ì í•©

**ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œ (Context Mode)**:
- ì‚¬ìš©ìê°€ ì§ì ‘ ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸ ì œê³µ
- ë„ë©”ì¸ë³„ íŠ¹í™” í‰ê°€ ê°€ëŠ¥
- ë” ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„

---

## ğŸ¤– Agent Types

### 1. Get Source Agent
- **ì—­í• **: Tavily ê²€ìƒ‰ì„ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ìš”ì•½
- **ì…ë ¥**: ê²€ì¦í•  ì§ˆì˜ + ê²€ìƒ‰ ê´€ì (pros/cons)
- **ì¶œë ¥**: ìš”ì•½ëœ ì»¨í…ìŠ¤íŠ¸ + ì°¸ì¡° URL ëª©ë¡

### 2. Context Consistency Agent 
- **ì—­í• **: NLI ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€
- **ì…ë ¥**: ì»¨í…ìŠ¤íŠ¸ + ê²€ì¦í•  ë¬¸ì¥
- **ì¶œë ¥**: í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜(0.0-1.0) + í‰ê°€ ì´ìœ 

### 3. Reason Summary Agent
- **ì—­í• **: ë‹¤ì¤‘ í‰ê°€ ê²°ê³¼ì˜ í•œêµ­ì–´ ìš”ì•½
- **ì…ë ¥**: ì—¬ëŸ¬ í‰ê°€ ì´ìœ  ëª©ë¡
- **ì¶œë ¥**: í†µí•©ëœ í•œêµ­ì–´ ìš”ì•½ (300ì ì´ë‚´)

---

## ğŸ“¡ A2A Client Guide

### client í…ŒìŠ¤íŠ¸ ì‹¤í–‰

í”„ë¡œì íŠ¸ì— í¬í•¨ëœ CLI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©:

```bash
# ê²€ìƒ‰ ëª¨ë“œë¡œ ì‹¤í–‰
uv run src/client/a2a_client.py --agent-type search

# ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰  
uv run src/client/a2a_client.py --agent-type context

# ì˜ˆì œ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©
uv run src/client/a2a_client.py --agent-type context --use-example-context
```

### A2A í´ë¼ì´ì–¸íŠ¸ ì‘ì„± (ì¶”ì²œ)

```python
import asyncio
from uuid import uuid4
import httpx
from a2a.client import A2AClient, A2ACardResolver
from a2a.types import Message, MessageSendParams, SendStreamingMessageRequest

class HallucinationCheckClient:
    def __init__(self, host="localhost", port=8008):
        self.base_url = f"http://{host}:{port}"
        
    async def check_with_search(self, statement):
        """ê²€ìƒ‰ ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì‚¬"""
        async with httpx.AsyncClient() as httpx_client:
            # A2A ì—ì´ì „íŠ¸ ì¹´ë“œ ê°€ì ¸ì˜¤ê¸°
            agent_card = await A2ACardResolver(
                httpx_client=httpx_client,
                base_url=self.base_url,
            ).get_agent_card()

            # A2A í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = A2AClient(
                httpx_client=httpx_client,
                agent_card=agent_card,
            )

            # ë©”ì‹œì§€ í˜ì´ë¡œë“œ êµ¬ì„±
            context_id = uuid4().hex
            send_message_payload = {
                "message": Message(
                    message_id=uuid4().hex,
                    context_id=context_id,
                    role="user",
                    parts=[{"type": "text", "text": statement}],
                ),
                "metadata": {
                    "enable_tavily_search_engine/v1": {"enable": True}
                }
            }

            # ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ìš”ì²­
            message_request = SendStreamingMessageRequest(
                id=uuid4().hex, 
                params=MessageSendParams(**send_message_payload)
            )

            # ì‘ë‹µ ì²˜ë¦¬
            async for chunk in client.send_message_streaming(message_request):
                chunk_data = chunk.model_dump(mode='json', exclude_none=True)
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if status := chunk_data.get("result", {}).get("status"):
                    print(f"Status: {status['state']}")
                    
                # ì™„ë£Œëœ ê²°ê³¼ ë°˜í™˜
                if chunk_data.get("result", {}).get("status", {}).get("state") == "completed":
                    return chunk_data["result"]

    async def check_with_context(self, statement, context):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì‚¬"""
        async with httpx.AsyncClient() as httpx_client:
            agent_card = await A2ACardResolver(
                httpx_client=httpx_client,
                base_url=self.base_url,
            ).get_agent_card()

            client = A2AClient(
                httpx_client=httpx_client,
                agent_card=agent_card,
            )

            context_id = uuid4().hex
            send_message_payload = {
                "message": Message(
                    message_id=uuid4().hex,
                    context_id=context_id,
                    role="user",
                    parts=[{"type": "text", "text": statement}],
                ),
                "metadata": {
                    "set_input_context_explicitly/v1": {"input_context": context}
                }
            }

            message_request = SendStreamingMessageRequest(
                id=uuid4().hex, 
                params=MessageSendParams(**send_message_payload)
            )

            async for chunk in client.send_message_streaming(message_request):
                chunk_data = chunk.model_dump(mode='json', exclude_none=True)
                if chunk_data.get("result", {}).get("status", {}).get("state") == "completed":
                    return chunk_data["result"]

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    client = HallucinationCheckClient()

    # ê²€ìƒ‰ ê¸°ë°˜ í‰ê°€
    print("=== ê²€ìƒ‰ ê¸°ë°˜ í‰ê°€ ===")
    result1 = await client.check_with_search("Tesla invented the light bulb")
    print(f"Result: {result1}")

    # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‰ê°€
    print("\n=== ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‰ê°€ ===")
    context = "Tesla was known for AC electrical systems and wireless technology."
    result2 = await client.check_with_context("Tesla invented the light bulb", context)
    print(f"Result: {result2}")

if __name__ == "__main__":
    asyncio.run(main())
```

---
